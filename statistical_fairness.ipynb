{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "statistical_fairness.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edtechequity/ml_fairness_toolkit/blob/master/statistical_fairness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnuv81Fu2_gU",
        "colab_type": "text"
      },
      "source": [
        "# Statistical Fairness\n",
        "\n",
        "This module is intended to provide a quick overview of methods for statistical fairness. We cover three common types, in increasing order of complexity. No single metric will alone ensure fairness, but it can be helpful to see how your model fares across them.\n",
        "- **Unawareness.**\n",
        "- **Demographic Parity.**\n",
        "- **Equalized Odds.**\n",
        "\n",
        "\n",
        "\n",
        "Statistical fairness is still an active area of research. If interested in a deeper dive, consider this [this tutorial](https://mrtz.org/nips17/#/) offered at NIPS (Neural Information Processing Systems Conference).\n",
        "\n",
        "This notebook will use the same COMPAS dataset used in the other two modules. You can download it from Kaggle [here](https://www.kaggle.com/danofer/compass/data#propublica_data_for_fairml.csv). The file we'll be using is `propublica_data_for_fairml.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJtDNgsk4fTq",
        "colab_type": "text"
      },
      "source": [
        "### Import Dataset\n",
        "\n",
        "First, let's import a dataset. We recommend using our sample Propublica dataset first, then retrying some of this analysis with your own data.\n",
        "\n",
        "You can upload your file using the \"upload\" button in the top left of Google Colab to begin. This code assumes a CSV or Pandas-compliant format.\n",
        "\n",
        "The code below imports the data from the uploaded file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26rVU9MU4pJ-",
        "colab_type": "code",
        "outputId": "569b354c-e237-4fbe-95e0-d572e2c2c4f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv('propublica_data_for_fairml.csv')\n",
        "df[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Two_yr_Recidivism</th>\n",
              "      <th>Number_of_Priors</th>\n",
              "      <th>score_factor</th>\n",
              "      <th>Age_Above_FourtyFive</th>\n",
              "      <th>Age_Below_TwentyFive</th>\n",
              "      <th>African_American</th>\n",
              "      <th>Asian</th>\n",
              "      <th>Hispanic</th>\n",
              "      <th>Native_American</th>\n",
              "      <th>Other</th>\n",
              "      <th>Female</th>\n",
              "      <th>Misdemeanor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Two_yr_Recidivism  Number_of_Priors  ...  Female  Misdemeanor\n",
              "0                  0                 0  ...       0            0\n",
              "1                  1                 0  ...       0            0\n",
              "2                  1                 4  ...       0            0\n",
              "3                  0                 0  ...       0            1\n",
              "4                  1                14  ...       0            0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX6tU0Sz473T",
        "colab_type": "text"
      },
      "source": [
        "# 1. Unawareness\n",
        "**Note: this is widely considered an insufficient method for measuring statistical fairness.**\n",
        "\n",
        "- **Intuition.** Unawareness means that the machine learning model does not take as a feature any protected attributes, like race and gender. \n",
        "- **Pros.**\n",
        "  - Simple and intuitive\n",
        "  - Often satisfies legal requirements (see [disparate treatment](https://en.wikipedia.org/wiki/Disparate_treatment))\n",
        "- **Cons.**\n",
        "  - Fails when the dataset has other features that are highly correlated with protected attributes (e.g., neighborhood and race). Check out our [Dataset notebook](https://github.com/edtechequity/ml_fairness_toolkit/blob/master/dataset.ipynb) for feature correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtLmuU-T6zHB",
        "colab_type": "text"
      },
      "source": [
        "## 2. Demographic Parity\n",
        "\n",
        "- **Intuition.** Demographic parity is satisfied when a classifier is equally likely to flag people in two groups (e.g., `African_American` and not).\n",
        "- **Math.** The probability of being flagged is independent of the protected attribute.\n",
        "\n",
        "- **Pros.**\n",
        "  - Fairly intuitive: you want your model to flag people at the same rate irrespective of certain protected categories.\n",
        "  - Often satisfies legal requirements (see [\"the 80% rule\"](https://en.wikipedia.org/wiki/Disparate_impact#The_80%_rule))\n",
        "- **Cons.**\n",
        "  - If applied without care, it can unfairly penalize certain groups. If older people are less likely to reoffend but a model is constrained by statistical parity, it will either incorrectly flag more of the elderly or incorrectly clear young people.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFSjPGzdAVo1",
        "colab_type": "text"
      },
      "source": [
        "### Demographic Parity Examples\n",
        "The code below calculates the rate at which in- and out-groups are flagged. \n",
        "\n",
        "\n",
        "- **Example 1: African American people.** The COMPAS algorithm flags African American people 58% of the time, compared to 31% otherwise. This means African Americans are flagged at almost twice the rate of everyone else. This does **not** satisfy \"the 80% rule\" for demographic parity. \n",
        "\n",
        "- **Example 2: Women.** The COMPAS algorithm flags women 41% of the time and men 46% of the time, which is far closer to satisfying demographic parity. This satisfies \"the 80% rule.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiknNRAq87j0",
        "colab_type": "code",
        "outputId": "70918ef8-60b7-41b0-a5ea-f862f31fdecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# The following code will count how many people of certain classes were flagged by the COMPAS algorithm.\n",
        "\n",
        "def print_positive_rates(df, prediction, feature):\n",
        "  in_class = df.apply(lambda d: d[feature] == 1, axis=1)\n",
        "  out_class = df.apply(lambda d: d[feature] == 0, axis=1)\n",
        "  in_flagged = df.apply(lambda d: d[prediction] == 1 and d[feature] == 1, axis=1)\n",
        "  out_flagged = df.apply(lambda d: d[prediction] == 1 and d[feature] == 0, axis=1)\n",
        "\n",
        "  num_in_class = len(in_class[in_class == True].index)\n",
        "  num_out_class = len(out_class[out_class == True].index)\n",
        "  num_in_flagged = len(in_flagged[in_flagged == True].index)\n",
        "  num_out_flagged = len(out_flagged[out_flagged == True].index)\n",
        "\n",
        "  pct_in_flagged = round(100 * num_in_flagged / num_in_class)\n",
        "  pct_out_flagged = round(100 * num_out_flagged / num_out_class)\n",
        "\n",
        "  print('In class flagged at a rate of ', pct_in_flagged, '%.', sep='')\n",
        "  print('Out class flagged at a rate of ', pct_out_flagged, '%.', sep='')\n",
        "\n",
        "  lower = min(pct_in_flagged, pct_out_flagged)\n",
        "  higher = max(pct_in_flagged, pct_out_flagged)\n",
        "  if lower / higher > .8:\n",
        "    print ('This passes \"the 80% rule\" of demographic parity.')\n",
        "  else:\n",
        "    print ('This DOES NOT pass \"the 80% rule\" of demographic parity.')\n",
        "\n",
        "print('*** Example 1: Demographic Parity Test for African American folks ***')\n",
        "print_positive_rates(df, 'score_factor', 'African_American')\n",
        "\n",
        "print('\\n*** Example 2: Demographic Parity Test for women ***')\n",
        "print_positive_rates(df, 'score_factor', 'Female')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Example 1: Demographic Parity Test for African American folks ***\n",
            "In class flagged at a rate of 58%.\n",
            "Out class flagged at a rate of 31%.\n",
            "This DOES NOT pass \"the 80% rule\" of demographic parity.\n",
            "\n",
            "*** Example 2: Demographic Parity Test for women ***\n",
            "In class flagged at a rate of 41%.\n",
            "Out class flagged at a rate of 46%.\n",
            "This passes \"the 80% rule\" of demographic parity.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyvJdpmCAxl_",
        "colab_type": "text"
      },
      "source": [
        "# 3. Equalized Odds\n",
        "- **Intuition.** Equalized odds is satisfied when a classifier is equally likely to flag people in two groups who didn't deserve to have been flagged. It's different from demographic parity because it takes into account whether each individual eventually reoffends.\n",
        "- **Math.** The probability of being flagged is independent of the protected category, given the final label (whether they should have been flagged).\n",
        "- **Pros.**\n",
        "  - This \"fixes\" a problem with demographic parity by allowing a model to flag certain groups at higher rates if they are actually more likely to reoffend. For example, you allow a model to flag people under 60 more than people over 60.\n",
        "  - It creates an incentive for the algorithm maker to reduce errors in the worst-off group first, because the model must perform equally \"bad\" for all classes.\n",
        "- **Cons.**\n",
        "  - This metric doesn't account for societal issues that cause certain groups to be more likely to merit flagging. If there's a societal reason women are less likely to reoffend, using equalized odds does nothing to help men reoffend less. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqw0J5-YEidr",
        "colab_type": "text"
      },
      "source": [
        "### Equalized Odds Examples\n",
        "\n",
        "The code below calculates the rate at which in- and out-groups who did not eventually reoffend were incorrectly flagged.\n",
        "\n",
        "- **Example 1: African American people.** The COMPAS algorithm incorrectly flags African American people who didn't go on to reoffend 42% of the time, compared to 20% otherwise. This means innocent African Americans are flagged at over twice the rate of everyone else. This does **not** come close to satisfying equalized odds.\n",
        "\n",
        "- **Example 2: Women.** The COMPAS algorithm incorrectly flags women who didn't go on to reoffend 30% of the time, the same as men. This does satisfy equalized odds.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whyvEWLAEg0O",
        "colab_type": "code",
        "outputId": "862a527f-0b92-4606-f46a-8839afb4aaf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "def print_conditional_positive_rates(df, prediction, label, feature):\n",
        "  in_class = df.apply(lambda d: d[feature] == 1 and d[label] == 0, axis=1)\n",
        "  out_class = df.apply(lambda d: d[feature] == 0 and d[label] == 0, axis=1)\n",
        "  in_flagged = df.apply(lambda d: d[prediction] == 1 and d[label] == 0 and d[feature] == 1, axis=1)\n",
        "  out_flagged = df.apply(lambda d: d[prediction] == 1 and d[label] == 0 and d[feature] == 0, axis=1)\n",
        "\n",
        "  num_in_class = len(in_class[in_class == True].index)\n",
        "  num_out_class = len(out_class[out_class == True].index)\n",
        "  num_in_flagged = len(in_flagged[in_flagged == True].index)\n",
        "  num_out_flagged = len(out_flagged[out_flagged == True].index)\n",
        "\n",
        "  pct_in_flagged = round(100 * num_in_flagged / num_in_class)\n",
        "  pct_out_flagged = round(100 * num_out_flagged / num_out_class)\n",
        "\n",
        "  print('In class flagged at a rate of ', pct_in_flagged, '%.', sep='')\n",
        "  print('Out class flagged at a rate of ', pct_out_flagged, '%.', sep='')\n",
        "\n",
        "  lower = min(pct_in_flagged, pct_out_flagged)\n",
        "  higher = max(pct_in_flagged, pct_out_flagged)\n",
        "  if lower / higher > .9:\n",
        "    print ('This passes the equalized odds test.')\n",
        "  else:\n",
        "    print ('This DOES NOT pass the equalized odds test.')\n",
        "\n",
        "print('*** Example 1: Equalized Odds Test for African American folks ***')\n",
        "print_conditional_positive_rates(df, 'score_factor', 'Two_yr_Recidivism', 'African_American')\n",
        "\n",
        "print('\\n*** Example 2: Equalized Odds Test for women ***')\n",
        "print_conditional_positive_rates(df, 'score_factor', 'Two_yr_Recidivism', 'Female')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Example 1: Equalized Odds Test for African American folks ***\n",
            "In class flagged at a rate of 42%.\n",
            "Out class flagged at a rate of 20%.\n",
            "This DOES NOT pass the equalized odds test.\n",
            "\n",
            "*** Example 2: Equalized Odds Test for women ***\n",
            "In class flagged at a rate of 30%.\n",
            "Out class flagged at a rate of 30%.\n",
            "This passes the equalized odds test.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkPu2_J1LzzO",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "No statistical fairness metric is perfect, and it's often mathematically impossible to perfectly satisfy all definitions at the same time. However, statistical fairness should be a part of the core metrics that your team monitors internally. This is the only way to ensure that you're creating an equitable product that remains equitable over time. "
      ]
    }
  ]
}